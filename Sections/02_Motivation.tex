
\section{Motivation}
\label{sec:Motivation}

\subsection{Why Explore Transfer Learning?}
One reason to explore transfer learning
is to study the nature of
generality in SE. Professional
societies assume such generalities exist when they
offer lists of supposedly general ``best practices'':
\squishlist
\item For example, the  IEEE 1012 standard for software
  verification~\cite{1012} proposes numerous methods for assessing software quality;
\item
 Endres and Rombach catalog
dozens of lessons of software
engineering~\cite{endres03} such as McCabe's Law (functions with a ``cyclomatic complexity'' greater
than ten are more error prone);
\item Further, many other
widely-cited researchers do the same such as Jones~\cite{jones10} and
Glass~\cite{glass02} who list (for exmple)
Brooks' Law (adding programmers to a late project makes it later).
% Boehm~\cite{hoehm00b}.
\item
  More generally, Budgen and Kitchenham seek to
reorganize SE research using general conclusions
drawn from a larger number of
studies~\cite{budgen06,budgen09}.
\squishend

Given the constant pace of change within SE, can we trust
those supposed generalities?
Numerous {\em local learning} results show that we
should mistrust general conclusions (made over a
wide population of projects) since they may not hold
for projects~\cite{me12d,betten14}. Posnett et al.~\cite{posnett11}
discuss {\em ecological inference} in software
engineering, which is the concept that what holds
for the entire population also holds for each
individual. They learn models at different levels
of aggregation (modules, packages, and files) and show
that models working at one level of aggregation
can be sub-optimal at others.  For example, Yang et
al.~\cite{yang11}, Bettenburg et
al.~\cite{betten14}, and Menzies et al.~\cite{me12d}
all explore the generation of models using {\em all}
data versus {\em local} samples that are more specific
to particular test cases. These papers report that
better models (sometimes with much lower variance in
their predictions) are generated from local
information.
These results have an unsettling effect on anyone
struggling to propose policies for an organization.
If all prior conclusions can change for the new
project, or some small part of a project, how can
any manager ever hope to propose and defend IT
policies (e.g., when should some module be inspected,
when should it be refactored, where to focus
expensive testing procedures, etc.)?

If we cannot {\em generalize} to all projects and all parts
of current projects, perhaps a more achievable goal is to {\em stabilize} the pace of conclusion change.
While it may be
a fool's errand and wait for eternal and global SE
conclusions, one possible approach is for organizations
to declare $N$ prior projects as {\em reference projects},
from which lessons learned will be transferred to new projects.
In practice, using such reference sets requires three processes:
\begin{enumerate}
\item Finding the reference sets (this paper shows that finding
  them may not require extensive and protracted data collection, at least for defect prediction).
  \item Recognizing when to update the reference set. In practice,
  this could be as simple as noting when predictions start failing for new projects---at which time, we would loop to the point 1).
\item Transferring
  lessons from the reference set to new projects.
\end{enumerate}
In the case where all the datasets use the same metrics, this is a relatively
simple task. Krishna et al.~\cite{krishna16} have found such reference projects just by training of
a project X then testing on a project Y (and the reference set are the project Xs with highest scores).
Once found, these reference sets can generate policies of an organization that are
stable just as long as the reference set is not updated.

In this paper, we do not address the pace of change in the reference set
(that is left for future work).
Rather, we focus on the point 3): transferring lessons from
the reference set to new projects in the case of heterogeneous data sets. To support this third point,
we need to resolve the problem
  that this paper addresses, i.e., data expressed in different terminology
  cannot transfer till there is enough data to match old projects to new projects.




\subsection{Why Explore Defect Prediction?}

There are many lessons we {\em might} try to transfer between projects
about staffing policies, testing methods, language choices, etc. While
all those matters are important and are worthy of research, this section
discusses why we focus on defect prediction.

Human programmers are clever, but flawed. Coding  adds functionality, but also defects.
Hence, software sometimes crashes (perhaps at the most awkward or dangerous moment) or delivers
the wrong functionality. For a very long list of software-related errors,
see  Peter Neumann's ``Risk Digest'' at http://catless.ncl.ac.uk/Risks.

Since programming inherently
introduces defects into  programs, it is important to test them before they're used.
Testing is expensive.
Software assessment budgets are finite
while assessment effectiveness increases
exponentially with assessment effort.
For example, for  black-box testing methods,
a {\em linear} increase
in the confidence $C$ of finding  defects
can take {\em exponentially} more effort.\footnote{A randomly selected
input to a program will find a fault with probability $p$.
After $N$ random black-box tests, the chances of the inputs
not revealing any fault
is $(1-p)^N$. Hence, the chances $C$ of seeing the fault is $1-(1-p)^N$
which can be rearranged to
 $N(C,p)=log(1 -
C)/log(1-p)$. For example, $N(0.90,10^{-3})=2301$
but $N(0.98,10^{-3})=3901$; i.e., nearly double the number of tests.}
Exponential costs quickly exhaust finite resources so
standard practice is to apply the best
available  methods on code sections that seem most critical.
But any method that focuses on parts of the code
can blind us to defects in other areas. Some  {\em lightweight sampling policy} should be used to explore the rest of the system.
This sampling policy will always be incomplete.
Nevertheless, it is the only option when
resources prevent a complete assessment of everything.

One such lightweight sampling policy is defect predictors learned from software metrics such as static code attributes.
For example, given static code descriptors for each module, plus a count of the number of issues raised during inspect (or at runtime),
data miners can learn where the probability of software defects is highest.


The rest of this section argues that such defect predictors are   {\em easy to
use}, {\em widely-used}, and {\em useful} to use.

{\em Easy to use:} Various software metrics such as static code attributes and process metrics can be automatically collected, even for very large systems, from software repositories~\cite{Basili96,Halstead77,McCabe76,nagappan05,Rahman13}.
Other methods, like  manual code reviews, are far slower and far more labor-intensive.
For example, depending on the review methods, 8 to 20 LOC/minute can be
inspected and this effort repeats for all members of the review team,
which can be as large as four or six people~\cite{me02f}.

{\em Widely used:}  Researchers and industrial practitioners  use the software metrics to guide software
quality predictions.
 Defect prediction models have been reported
  at large industrial companies such as Google~\cite{lewis13}, Microsoft~\cite{Nagappan06}, AT\&T~\cite{Ostrand05}, and Samsung~\cite{Kim15remi}.
Verification and validation (V\&V) textbooks
\cite{rakitin01} advise using the software metrics
to decide which modules are worth manual inspections.


{\em Useful:}
Defect predictors often  find the location of  70\% (or more)
of the defects in code~\cite{me07b}.
Defect predictors have some level of generality:
predictors learned at NASA~\cite{me07b} have also been found useful elsewhere
(e.g., in Turkey~\cite{tosun10,tosun09}).
The success of this method in  predictors in finding bugs is markedly
higher than other currently-used
industrial
methods such as manual code reviews. For example,
a  panel at {\em IEEE Metrics
2002}~\cite{shu02} concluded that manual software  reviews can find ${\approx} 60\%$
of defects.
In another work,
Raffo documents the typical defect detection capability of
industrial review methods:   around 50\%
 for full Fagan inspections~\cite{fagan76} to
21\% for less-structured inspections.
In some sense, defect prediction might not be necessary for small software projects.
However, software projects seldom grow by small fractions in practice. For example, a project team may suddenly merge a large branch into a master branch in a version control system or add a large third-part library. In addition, a small project could be just one of many other projects in a software company. In this case, the small project also should be considered for limited resource allocation in terms of software quality control by the company. For this reason, defect prediction could be useful even for the small software projects in practice.

Not only do defect predictors perform well compared to manual methods,
they also are competitive with certain automatic methods.
A recent study at ICSE'14, Rahman et al.~\cite{rahman14:icse} compared
(a) static code analysis tools FindBugs, Jlint, and Pmd and (b) defect predictors
(which they called ``statistical defect prediction'') built using logistic regression.
They found  no significant differences in the cost-effectiveness
of these  approaches. Given this equivalence, it is significant to note that
defect prediction can be quickly adapted to new languages by building lightweight
parsers to extract high-level software metrics. The same is not true for static code analyzers---these need extensive modification before they can be used on new
languages.

Having offered general high-level notes on defect prediction,
the next section describes in detail the related work on this topic.
